lat_dim: 128
norm: batch
spectral_norm: false
self_attention: false
activation: elu
leak: 0.1

generator:
  hidden_dim: 32
  activation: relu
  norm: batch
  depth: 6
  attention_layers: 

discriminator: 
  hidden_dim: 32
  activation: elu
  norm: 
  spectral_norm: false
  depth: 6
