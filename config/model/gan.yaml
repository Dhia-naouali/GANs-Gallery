name: GAN
lat_dim: 256
norm: batch
spectral_norm: false
self_attention: false
activation: relu
leak: 0.2

generator:
  channels: [512, 512, 256, 256, 128, 3]
  activation: relu
  norm: batch
  attention_layers: []

discriminator: 
  activation: elu
  norm: none
  spectral_norm: false
  channels: [3, 64, 128, 128, 256, 256]